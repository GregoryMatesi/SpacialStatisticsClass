---
title: "Hw05 - Regional Count Data Homework (Spatial Autocorrelation)"
author: "Gregory Matesi"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}
rm(list = ls())
load("~../../Desktop/SpatialStatisticsClass/chapter7/nc.rda")
ncdf <- as.data.frame(nc.sids)
library(sf)
library(smerc)
library(spdep)
```

# Problem 1

The `nc.rda` file contains information related to SIDS cases in North Carolina. The object includes `nc.sids`, a spatial data frame compatible with the **sf** package with 20 variables measured for 100 counties. It contains data given in Cressie (1991, pp. 386-9), Cressie and Read (1985), and Cressie and Chan (1989) on sudden infant deaths in North Carolina for 1974-78 and 1979-84. The `nc.rda` objects also contains the neighbor list given by Cressie and Chan (1989) omitting self-neighbors (`nb89`), and the neighbor list given by Cressie and Read (1985) for contiguities (`nb85`).  The `nc.sids` object contains the following variables:

* `SP_ID`: SpatialPolygons ID
* `CNTY_ID`: county ID
* `east`: eastings, county seat, miles, local projection
* `north`: northings, county seat, miles, local projection
* `L_id`: Cressie and Read (1985) L index
* `M_id`: Cressie and Read (1985) M index
* `names`: County names
* `AREA`: County polygon areas in degree units
* `PERIMETER`: County polygon perimeters in degree units
* `CNTY_` Internal county ID
* `NAME`: County names
* `FIPS`: County ID
* `FIPSNO`: County ID
* `CRESS_ID`: Cressie papers ID
* `BIR74`: births, 1974-78
* `SID74`: SID deaths, 1974-78
* `NWBIR74`: non-white births, 1974-78
* `BIR79`: births, 1979-84
* `SID79`: SID deaths, 1979-84
* `NWBIR79`: non-white births, 1979-84

## (a)
Use Moran’s I to test whether there is evidence of positive spatial autocorrelation for the `SID74` variable under the normal assumption.  Use a binary weights matrix for the `nb85` neighbor relationship.

**Solution**

```{r}
w = nb2mat(nb85, style = "B")
# see ?nb2listw for more options
# proximaty matrix in list format
lw = nb2listw(nb85, style = "B")

# base test w/ normality approximation for p-value
moran.test(nc.sids$SID74, listw = lw, randomisation = FALSE)
```

## (b)
Use Moran’s I to test whether there is evidence of positive spatial autocorrelation for the `SID74` variable under the randomization assumption.  Use a binary weights matrix for the `nb85` neighbor relationship.

**Solution**

```{r}
# base test w/ randomization assumption 
moran.test(nc.sids$SID74, listw = lw, randomisation = TRUE)
```

## (c)
Use the Moran’s I statsitic to test whether there is evidence of positive spatial autocorrelation for the `SID74` variable under the CRH. Note: use the standard Moran's I statistic, but use a Monte Carlo test to test the constant risk hypothesis. Use a binary weights matrix for the `nb85` neighbor relationship and 499 simulated data sets. Use the `BIR74` variable for the population size of each region.

**Solution**

```{r}

# base test w/ randomization p-value
(ir = moran.mc(nc.sids$SID74, listw = lw, nsim = 499))
# base test w/ Monto Carlo p-value, simulating data under constant risk hypothesis
# some preliminaries
N = length(nc.sids$SID74) # number of regions
y = nc.sids$SID74 # number of cases
n = nc.sids$BIR74 #population sizes
r <- sum(y)/sum(n) # estimated risk
rni <- r * n # expected per region

# observed moran's statistic
nsim = 499
t0 = moran(y, listw = lw, n = N, S0 = Szero(lw))$I
# simulate data under CRH
tsim = numeric(nsim)
# calculate moran's i for poisson data simulated under crh
for (i in 1:nsim) {
  tsim[i] = moran(rpois(N, rni), listw = lw, n = N, S0 = Szero(lw))$I
}
```

## (d)
Use the constant risk version of Moran’s I (Walter 1992) to test whether there is evidence of positive spatial autocorrelation for the `SID74` variable under the CRH. Use a binary weights matrix for the `nb85` neighbor relationship and 499 simulated data sets. Use the `BIR74` variable for the population size of each region.

**Solution**

```{r}

### Use CR Moran's I for inference
# make a function out of this process
i_cr = function(y, rni, w) {
  y_std = matrix((y - rni)/sqrt(rni))
  return(sum(w * y_std %*% t(y_std))/sum(w))
}

tsimc = numeric(nsim)
t0c = i_cr(y, rni, w) # observed statistic
# statistics for data simualted under CRH
for (i in 1:nsim) tsimc[i] = i_cr(rpois(N, rni), rni = rni, w = w)
# p-value
(sum(tsimc >= t0c) + 1)/(nsim + 1)

```

## (e)

How does the Moran’s I result change if we use the constant risk version of Moran’s I along with the constant risk hypothesis (instead of the standard Moran’s I statistic with the CRH)?  Why does this change occur?

**Solution**

If we use the standard Moran's I statistic with Constant Risk Hypothesis, we obtain a moderately low p-value of around 0.02. If we use the Constant Risk version of Moran's I, we obtain a much lower p-value of 0.002. If our rejection threshold was 0.01, we would reject the null hypothesis for the CR version of Moran's I but not for the standard version under CRH.

This change occurs because




## (f)

The intercentroid distances for the North Carolina data are between 0.12 and 8.22 units.  In the context of Tango’s recommended weights matrix, a very weak spatial correlation has $\kappa=0.1$ and a very strong spatial correlation has $\kappa=7$. Perform Monte Carlo tests using using Tango's index with Tango's recommended weights with both $\kappa=0.1$ and $\kappa=7$ for the `SID74` variable with 499 simulated data sets.  For the centroid coordinates, use the `east` and `north` columns of `nc.sids`. Interpret your results in the context of the problem.

**Solution**

```{r}
rm(list = ls())
load("~../../Desktop/SpatialStatisticsClass/chapter7/nc.rda")

coords = as.matrix(cbind(nc.sids$east, nc.sids$north))
cases = nc.sids$SID74
pop = nc.sids$BIR74

d = as.matrix(dist(coords))

w1  <- dweights(coords, kappa = 1)
w7  <- dweights(coords, kappa = 7)

(tango_1  <- tango.test(cases, pop, w1))
(tango_7  <- tango.test(cases, pop, w7))

gof <- c(tango_1$gof,tango_7$gof)
sa <- c(tango_1$sa,tango_7$sa)
plot(gof, sa)

(tango_mc1 <-  tango.test(cases, pop, w1, nsim = 499))
psim1 <- (1 + sum(tango_mc1$tstat.sim >= tango_mc1$tstat))/(1 + 499)
print(paste("chi-square p: ", round(tango_1$pvalue.chisq,5),
            ", MC p: ",round(psim1, 5)))

(tango_mc7 <-  tango.test(cases, pop, w7, nsim = 499))
psim7 <- (1 + sum(tango_mc7$tstat.sim >= tango_mc7$tstat))/(1 + 499)
print(paste("chi-square p: ", round(tango_7$pvalue.chisq,5),
            ", MC p: ",round(psim7, 5)))
```

## (g)

Compare the goodness-of-fit and spatial autocorrelation components of Tango’s statistic for the observed and simulated data in a plot.  (Do this for both values of $\kappa$).   Are the patterns similar for the weak versus strong spatial autocorrelation?  Or does the value of $\kappa$ dramatically impact the relative importance of the goodness-of-fit and spatial autocorrelation components?

**Solution**

```{r}
plot(tango_mc1)
# how extreme is each observed component
# compared to what we expect under the CRH
hist(tango_mc1$gof.sim, xlim = range(c(tango_mc1$gof.sim, tango_mc1$gof)))
abline(v = tango_mc1$gof)
hist(tango_mc1$sa.sim, xlim = range(c(tango_mc1$sa.sim, tango_mc1$sa)))
abline(v = tango_mc1$sa)
```

```{r}
plot(tango_mc1)
# how extreme is each observed component
# compared to what we expect under the CRH
hist(tango_mc1$gof.sim, xlim = range(c(tango_mc1$gof.sim, tango_mc1$gof)))
abline(v = tango_mc1$gof)
hist(tango_mc1$sa.sim, xlim = range(c(tango_mc1$sa.sim, tango_mc1$sa)))
abline(v = tango_mc1$sa)

```

# Problem 2

In this problem you are going to implement a portion of the spatial scan method. You can only use functions/packages loaded with by default by R (`stats`, `graphics`, `grDevices`, `utils`, `datasets`, `methods`, `base`). If you don't have to load a package to access the functionality, then you should be okay.

Suppose you have regional count data with the following characteristics:

```{r}
rm(list = ls())
```

```{r, include = FALSE}
region_id <- seq_len(4)
x <- c(1, 1, 2, 1.75)
y <- c(2, 1, 2.01, 1.75)
cases <- c(1, 2, 3, 2)
pop <- c(5, 3, 8, 4)
dtf <- data.frame(region_id, x, y, cases, population = pop)
```
```{r, echo = FALSE}
knitr::kable(dtf)
```

(`x`, `y`) define the centroid of each region.

Calculate the Poisson spatial scan statistic under the CRH assuming the constraint that no more than half the total population can be in a potential cluster/window. 

Break up your solution into parts:

## (a)
Compute the inter-centroid distance matrix between all centroids. Return the sample mean of this matrix.

**Solution**

```{r}
(D <- as.matrix( dist( cbind(x,y) )))
(mean.d <- mean(D))
```

## (b)
Using the distance matrix above, determine all possible windows (in terms of the region ids each window includes) in terms of nearest neighbors (the largest would have 3 non-inclusive neighbors). Print the complete list of windows.

**Solution**

```{r}
n <- length(pop)

my.w <- matrix(nrow = n, ncol = n)
my.w[,1] <- c(1,2,3,4)
my.w[,2] <- c(which(D[1,]== sort(D[1,])[2]),
           which(D[2,]== sort(D[2,])[2]),
           which(D[3,]== sort(D[3,])[2]),
           which(D[4,]== sort(D[4,])[2]))

my.w[,3] <- c(which(D[1,]== sort(D[1,])[3]),
           which(D[2,]== sort(D[2,])[3]),
           which(D[3,]== sort(D[3,])[3]),
           which(D[4,]== sort(D[4,])[3]))

my.w[,4] <- c(which(D[1,]== sort(D[1,])[4]),
           which(D[2,]== sort(D[2,])[4]),
           which(D[3,]== sort(D[3,])[4]),
           which(D[4,]== sort(D[4,])[4]))
window.indices <- list()

w <- matrix(nrow = n, ncol = n)
for (j in 1:n) {
  for (i in 1:n) {
    w[i,j] <- which(D[i,]== sort(D[i,])[j])
  }
}
all(my.w==w)
w

my.windows <- list()
cols <- rep(1:n, times=n)
rows <- rep(1:n, each=n)
for(i in 1:n**2){
  my.windows[[i]] <- w[rows[i], 1:cols[i]]
}
my.windows


```

## (c)
Determine the population size of each window to identify which windows have less than 50% of the total population. Print the population of each window

**Solution**

```{r}
half.pop <- sum(pop)/2

winpops <- data.frame(index=1:16)
for(i in 1:n**2){
  winpops$pop[i] <- sum(pop[my.windows[[i]]])
  winpops$wins[i] <- I(list(my.windows[[i]]))
}
winpops$trufals <- winpops$pop <= half.pop

winpops
```

## (d)
Only retain the windows that satisfy the population constraint. Print the list of retained windows.

**Solution**

```{r}
new.winpops <- winpops[which(winpops$trufals),]
new.winpops$wins
```

## (e)
For each remaining window, compute $Y_{in}$, $Y_{out}$, $E_{in}$, $E_{out}$. Print a data frame/matrix with the columns $Y_{in}$, $Y_{out}$, $E_{in}$, $E_{out}$.

**Solution**

```{r}
statistic.vals <- setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("y_in", "y_out", "e_in", "e_out"))
total_cases <- sum(cases)
total_pop <- sum(pop)
for(i in 1:length(new.winpops$wins)){
  popin <- sum(pop[new.winpops$wins[[i]]])
  popout <- sum(pop[-new.winpops$wins[[i]]])
  casein <- sum(cases[new.winpops$wins[[i]]])
  caseout <- sum(cases[-new.winpops$wins[[i]]])
  ein <- total_cases*(popin/total_pop)
  eout <- total_cases*(popout/total_pop)
  
  
  statistic.vals[i,] <- c(casein, caseout, ein, eout)
  
  
  
}
statistic.vals
```

## (f)
Compute the statistic
$\left(\frac{Y_{in}}{E_{in}}\right)^{Y_{in}} \left(\frac{Y_{out}}{E_{out}}\right)^{Y_{out}} I\left(\frac{Y_{in}}{E_{in}} \geq \frac{Y_{out}}{E_{out}}\right)$ 
for each remaining window.

**Solution**

```{r}
attach(statistic.vals)

for(i in 1:length(new.winpops$wins)){
  print((y_in[i] / e_in[i]) > (y_out[i] / e_out[i]))
    
}


for(i in 1:length(new.winpops$wins)){
  if( (y_in[i] / e_in[i]) > (y_out[i] / e_out[i]) ){
    print( ( ( (y_in[i]) / (e_in[i]) ) ** y_in[i] ) * 
           ( ( (y_out[i]) / (e_out[i]) ) ** y_out[i] )
    )
  }else{print(NA)}
}
```

